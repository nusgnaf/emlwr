# Introduction {#sec-intro}

```{r}
#| label: setup-common-01
#| include: false
source("includes/common.R")
```

```{r}
#| label: setup-01
#| include: false
if (!identical(Sys.getenv("emlwr.eval_fits"), "true")) {
  eval_fits <- FALSE
} else {
  eval_fits <- TRUE
}
```

::: callout-note
While this chapter is an early draft, it's relatively complete and should be coherent for readers.
:::

## Demonstration {#sec-demonstration}

To demonstrate the massive impact that a few small pieces of intuition can have on execution time when evaluating machine learning models, I'll run through a quick model tuning example. On the first go, I'll lean on tidymodels' default values and a simple grid search, and on the second, I'll pull a few tricks out from my sleeves that will drastically reduce the time to evaluate models while only negligibly decreasing predictive performance.

### Setup {#sec-setup}

First, loading a few needed packages:

```{r}
#| label: load-pkgs
library(tidymodels)
library(future)
library(finetune)
library(bonsai)
```

For the purposes of this example, we'll simulate a data set of 100,000 rows and 18 columns. The first column, `class`, is a binary outcome, and the remaining variables are a mix of numerics and factors.

```{r}
#| label: d-print
set.seed(1)
d <- simulate_classification(1e5)
d
```

See @sec-datasets for more information on this dataset.

We'll first split the data into training and testing sets before generating a set of 10 folds from the training data for cross-validation.

```{r}
#| label: d-split
set.seed(1)
d_split <- initial_split(d)
d_train <- training(d_split)
d_test <- testing(d_split)
d_folds <- vfold_cv(d_train)
```

### A first go {#sec-first-go}

For my first go at tuning, I'll tune a boosted tree model using grid search. By default, tidymodels will use XGBoost as the modeling engine. I'll try out a few different values for `learn_rate`—a parameter that controls how drastically newly added trees impact predictions—and `trees`—the number of trees in the ensemble.

```{r}
#| label: bt
bt <- 
  boost_tree(learn_rate = tune(), trees = tune()) %>%
  set_mode("classification")
```

I'll carry out a grid search using `tune_grid()`, trying out a bunch of different pairs of values for `learn_rate` and `trees` and seeing what sticks. The argument `grid = 12` indicates that I want to try out 12 different combinations of values and will let tidymodels take care of exactly what those values are.

```{r}
#| label: bm-basic
#| eval: !expr eval_fits
set.seed(1)

bm_basic <- 
  bench::mark(
    basic = 
      tune_grid(
        object = bt,
        preprocessor = class ~ .,
        resamples = d_folds,
        grid = 12
      )
  )
```

```{r}
#| label: get-bm-basic
#| include: false
if (identical(eval_fits, "true")) {
  bm_basic <- trim_bench_mark(bm_basic)
  qsave(bm_basic, file = "data/intro/bm_basic.rds")
} else {
  bm_basic <- qread("data/intro/bm_basic.rds")
}
```

`bench::mark()` returns, among other things, a precise timing of how long this process takes.

```{r}
#| label: bm-basic-print
bm_basic
```

Holy smokes! `r round(as.numeric(bm_basic$median[[1]]) / 60 / 60, 2)` hours is a good while. What all did `tune_grid()` do, though? First, let's break down how many model fits actually happened. Since I've supplied `grid = 12`, we're evaluating 12 possible model configurations. Each of those model configurations is evaluated against `d_folds`, a `r nrow(d_folds)`-fold cross validation object, meaning that each configuration is fitted `r nrow(d_folds)` times. That's `r nrow(d_folds) * 12` model fits! Further, consider that those fits happen on `r nrow(d_folds)-1`/`r nrow(d_folds)`ths of the training data, or `r nrow(d_folds$splits[[1]])` rows.

With a couple small changes, though, the time to tune this model can be *drastically* decreased.

### A speedy go {#sec-speedy-go}

To cut down on the time to evaluate these models, I'll make 4 small modifications that require something like 7 lines of code.

First, I'll **evaluate in parallel**: Almost all modern laptops have more than one CPU core, and distributing computations across them only takes 1 line of code with tidymodels.

```{r}
plan(multisession, workers = 4)
```

While this tuning process could benefit from distributing across many more cores than 4, I'll just use 4 here to give a realistic picture of the kinds of speedups possible on a typical laptop.

::: callout-note
Parallelism is the subject of @sec-parallel-computing.
:::

Then, we'll **use a clever grid**; the tidymodels framework enables something called the "submodel trick," a technique that will allow us to predict from many more models than we actually fit. Instead of just supplying `grid = 12` and letting tidymodels generate the grid automatically, I'll construct the grid myself.

```{r}
set.seed(1)
bt_grid <- bt %>%
  extract_parameter_set_dials() %>% 
  grid_regular(levels = 4)
```

::: callout-note
To learn more about the submodel trick, see @sec-submodel.
:::

Next, I'll **switch out the computational engine**: Substituting XGBoost with another gradient-boosting model that can better handle some properties of this dataset will cut down on our fit time by a good bit.

```{r}
bt_lgb <- bt %>% set_engine("lightgbm")
```

::: callout-note
@sec-models contains benchmarks and notes on the scaling properties of many of the modeling engines supported by tidymodels.
:::

Finally, I'll **give up early on poorly-performing models**: Rather than using grid search with `tune_grid()`, I'll use a technique called *racing* that stops evaluating models when they seem to be performing poorly using the `tune_race_anova()` function.

```{r}
#| label: bm-speedy
#| eval: !expr eval_fits
set.seed(1)

bm_speedy <- 
  bench::mark(
    speedy = 
      tune_race_anova(
        object = bt_lgb,
        preprocessor = class ~ .,
        resamples = d_folds,
        grid = bt_grid
      )
  )
```

```{r}
#| label: get-bm-speedy
#| include: false
if (identical(eval_fits, "true")) {
  bm_speedy <- trim_bench_mark(bm_speedy)
  qsave(bm_speedy, file = "data/intro/bm_speedy.rds")
} else {
  bm_speedy <- qread("data/intro/bm_speedy.rds")
}
```

::: callout-note
Alternative search strategies like racing are covered in detail in @sec-search.
:::

Checking out the new benchmarks:

```{r}
#| label: bm-speedy-print
bm_speedy
```

```{r}
#| label: back-to-default
#| echo: false
plan("default")
```

The total time to tune was reduced from `r round(as.numeric(bm_basic$median[[1]]) / 60 / 60, 2)` *hours* to `r round(as.numeric(bm_speedy$median[[1]]) / 60, 2)` *minutes*—the second approach was `r round(as.numeric(bm_basic[["median"]][[1]]) / as.numeric(bm_speedy[["median"]][[1]]))` times faster than the first.

The first thing I'd wonder when seeing this result is how much of a penalty in predictive performance I'd suffer due to this transition. Let's evaluate both of the top models from these tuning results on the test set. First, for the basic workflow:

```{r}
#| label: fit-basic
#| eval: !expr eval_fits
fit_basic <- 
  select_best(bm_basic$result[[1]], metric = "roc_auc") %>%
  finalize_workflow(workflow(class ~ ., bt), parameters = .) %>%
  last_fit(split = d_split)
```

```{r}
#| label: get-fit-basic
#| include: false
if (identical(eval_fits, "true")) {
  qsave(fit_basic, file = "data/intro/fit_basic.rds")
} else {
  fit_basic <- qread("data/intro/fit_basic.rds")
}
```

```{r}
#| label: print-fit-basic-metrics
collect_metrics(fit_basic)
```

As for the quicker approach:

```{r}
#| label: fit-speedy
#| eval: !expr eval_fits
fit_speedy <- 
  select_best(bm_speedy$result[[1]], metric = "roc_auc") %>%
  finalize_workflow(workflow(class ~ ., bt), parameters = .) %>%
  last_fit(split = d_split)
```

```{r}
#| label: get-fit-speedy
#| include: false
if (identical(eval_fits, "true")) {
  qsave(fit_speedy, file = "data/intro/fit_speedy.rds")
} else {
  fit_speedy <- qread("data/intro/fit_speedy.rds")
}
```

```{r}
#| label: print-fit-speedy-metrics
collect_metrics(fit_speedy)
```

Virtually indistinguishable performance results in `r round(100 * as.numeric(bm_speedy[["median"]][[1]]) / as.numeric(bm_basic[["median"]][[1]]), 1)`% of the time.

## Our approach

This book is intended for tidymodels users who have been waiting too long for their code to run. I generally assume that users are familiar with data manipulation and visualization with the tidyverse as well as the basics of machine learning with tidymodels, like evaluating models against resamples using performance metrics. For the former, I recommend [@wickham2023] for getting up to speed—for the latter, [@kuhn2022]. If you're generally comfortable with the content in those books, you're ready to go.

Modern laptops are remarkable. Users of tidymodels working on many machines made in the last few years are well-prepared to interactively develop machine learning models based on tens of millions of rows of data. That said, without the right information, it's quite easy to mistakenly introduce performance issues that result in analyses on even tens of thousands of rows of data becoming too cumbersome to work with. Generally, the tidymodels framework attempts to guard users from making such mistakes and addressing them ourselves when they're in our control. At the same time, many foundational and well-used approaches in classical machine learning have well-theorized adaptations that substantially cut down on the elapsed time while preserving predictive performance. The tidymodels framework implements many such adaptations and this book aims to surface them in a holistic and coherent way. Readers will come out of having read this book with a grab bag of one-liners that can cut down on elapsed time to develop machine learning models by orders of magnitude.

<!-- ## The cost of slowness -->

<!-- All of this said, R is not known for its computational efficiency. If I really prioritize that, why am I writing a book about R? -->

<!-- (i use R for many reasons. you evidently do, too. it is true that many modeling engines only implement some performance optimizations / hardware accelators for their python interfaces—at the same time, modeling engines in R are often interfaces to the same compiled code as that used from python.) <!--#  -->

## The hard part

To better understand how to cut down on the time to evaluate models with tidymodels, we need to understand a bit about how tidymodels works.

Like many other "unifying frameworks" for ML (mlr3, caret, scikit-learn(?)), the tidymodels framework itself does not implement the algorithms to train and predict from models. Instead, tidymodels provides a common interface to modeling *engines*: packages (or functions from packages) that provide the methods to `fit()` and `predict()`.

![A diagram representing portions of the elapsed time to fit a machine learning model with tidymodels. The central portion, shown in green, represents the time required by the modeling engine to fit or predict the model algorithm. The portions in green, on either side of the modeling engine, represent tidymodels' "overhead."](figures/translate_diagram.png){#fig-fit-boost-tree fig-alt="A timeline-ish diagram consisting of three portions in green, orange, and green again. The first portion is labeled \"translate input to engine code,\" the second \"call engine code,\" the third \"translate engine output.\" Above the timeline is the code required to fit a boosted tree with tidymodels." fig-align="center"}

The process of "translating" between the tidymodels and engine formats is illustrated in @fig-fit-boost-tree. When fitting and predicting with tidymodels, some portion of the elapsed time to run code is due to the "translation" of the inputted unified code to the specific syntax that the engine expects, and some portion of it is due to the translation of what the engine returns to the unified output returned by tidymodels; these portions are in the tidymodels team's control. The rest of the elapsed time occurs inside of the modeling engine's code.

The portions of the elapsed time that are in the tidymodels team's control are shown in green, and I'll refer to them in this book as "overhead." The overhead of tidymodels in terms of elapsed time is relatively constant with respect to the size of training data. This overhead consists of tasks like checking data types, handling errors and warnings, and—most importantly—programmatically assembling calls to engine functions.

The portion of the elapsed time shown in orange represents the actual training of (or predicting from) the model. This portion is implemented by the modeling *engine* and is thus not in the tidymodels team's control. In contrast to overhead, the elapsed time of this code is very much sensitive to the size of the inputted data; depending on the engine, increases in the number of rows or columns of training or testing data may drastically increase the time to train or predict from a given model.

::: callout-note
The algorithmic complexity of the models implemented by these engines is well-understood in many cases. At the same time, the behavior of elapsed time for some engine implementations often differs greatly from what theory would lead one to believe. Regressions in modeling code may introduce undue slowdowns and, conversely, performance optimizations that lead to elapsed times that scale better than theory would suggest may be the very reason for the existence of some engines.
:::

As shown in @fig-fit-scale, the proportion of elapsed time that overhead is responsible for depends on how quickly the engine can fit or predict for a given dataset.

![As a proportion, tidymodels' overhead is a function of how long an engine takes to fit. For a very simple fit on a small dataset, tidymodels overhead is quite substantial, but for even moderately complex model fits and/or moderately large data, the proportion overhead is negligible.](figures/scaling_diagram.png){#fig-fit-scale fig-alt="A diagram similar to the one from above, except three of them are stacked on top of each other. The length of the green portions is the same in each of the diagrams, but the orange portions are either very small, moderate, or long."}

Since the absolute overhead of tidymodels' translation is relatively constant, overhead is only a substantial portion of elapsed time when models fit or predict *very* quickly. For a linear model fitted on 30 data points with `lm()`, this overhead is [continuously benchmarked](https://github.com/tidymodels/parsnip/blob/11a3ab942f131e9d612d4d37c4b77273be064aaf/tests/testthat/test_fit_interfaces.R#L171) to remain under 2/3. That is, absolute worst-case, fitting a model with tidymodels takes three times longer than using the engine interface itself. However, this overhead approaches fractions of a percent for fits on even 10,000 rows for many engines. Thus, a focus on reducing the elapsed time of overhead is valuable in the sense that the framework ought not to unintentionally introduce regressions that cause overhead to scale with the size of training data, but in general, the hard part of reducing elapsed time when evaluating models is reducing the elapsed time for computations carried out by the modeling engine.

The next question is then *how could tidymodels cut down on elapsed time for modeling engines that it doesn't own?* To answer this question, let's revisit the applied example from @sec-first-go. In that first example, the code does some translation to the engine's syntax, sets up some error handling, and then fits and predicts from `r nrow(d_folds) * 12` models.

![A depiction of the timeline when tuning a model with tidymodels. The one, central orange segment representing engine code is now 120 individual segments, one for each model fit.](figures/basic_resample.png){#fig-basic-resample fig-alt="A similar diagram to the first, except that the middle orange portion is now cut up into 120 portions. When resampling models with tidymodels, the translation steps only need to happen once, and then the engine code is called for every data subset and hyperparameter combination."}

@fig-basic-resample depicts this process, where we evaluate all `r nrow(d_folds) * 12` models in order. Each white dot in the engine portion of the elapsed time represents another round of fitting and predicting with engine. Remember that in reality, for even modest dataset sizes, the green portions representing tidymodels overhead are much smaller by proportion than represented.

In @sec-speedy-go, the first thing I did was introduce a parallel backend. Distributing engine fits across available cores is itself a gamechanger, as illustrated in @fig-parallel-resample.

![Instead of fitting each of the 120 models in sequence, parallelism allows us to concurrently fit as many models as we have CPU cores.](figures/parallel_resample.png){#fig-parallel-resample fig-alt="The same diagram as above, except the portion of the timeline in orange is now split up into 4 segments and \"stacked\" on top of each other. The width of the diagram is 1/4 the size of what it was before."}

Then, switching out computational engines for a more performant alternative further reduces elapsed time, as shown in @fig-parallel-resample-opt.

![Depending on the modeling context, certain modeling engines can evaluate much more quickly than others.](figures/parallel_resample_opt.png){#fig-parallel-resample-opt fig-alt="The same diagram as above, though now each orange segment is half as wide as it was before. The width of the diagram is now half as wide as it was previously."}

Finally, as depicted in @fig-parallel-resample-opt2, the submodel trick described in @sec-submodel and racing described in @sec-search eliminate a substantial portion of the engine fits.

![Through racing and the submodel trick, we can evaluate the same number of potential models as before while fitting many fewer models.](figures/parallel_resample_opt2.png){#fig-parallel-resample-opt2 fig-alt="The same diagram as before, except around 2/3 of the orange segments have been removed."}

The tidymodels team devotes substantial energy to ensuring support for the most performant parallelization technologies, modeling engines, model-specific optimizations, and search techniques. This book will demonstrate how to best make use of these features to reduce the time needed to evaluate machine learning models.

## Datasets {#sec-datasets}

In @sec-setup, I used a function `simulate_classification()` to generate data. This is one of two functions, the other being `simulate_regression()`, that create the data underlying many of the experiments in this book.

These two functions are adaptations of their similarly named friends `sim_classification()` and `sim_regression()` from the modeldata package. They make small changes to those function—namely, introducing factor predictors with some tricky distributions—that surface slowdowns with some modeling engines.

Provided a number of rows, `sim_classification()` generates a tibble with that many rows and 16 columns:

```{r}
#| label: print-simulate-classification
d_class <- simulate_classification(1000)

d_class
```

The leftmost column, `class`, is a binary outcome variable, and the remaining columns are predictor variables. The predictors throw a few curveballs at the modeling functions we'll benchmark in this book.

For one, the predictors are moderately correlated. Correlated predictors can lead to unstable parameter estimates and can make the model more sensitive to small changes in the data. Further, correlated predictors may lead to slower convergence in gradient descent processes (like those driving gradient-boosted trees like XGBoost and LightGBM), as the resulting elongated and narrow surface of loss functions causes the algorithm to zigzag towards the optimum value, significantly increasing the training time along the way.

```{r}
#| label: correlate-d-class
#| echo: false
library(corrr)

correlate(d_class) %>% 
  autoplot(low = "#c46938", high = "#3d6c56")
```

Secondly, there are a number of factor predictors. Some modeling engines experience slower training times with many factor predictors for a variety of reasons. For one, most modeling engines ultimately implement training routines on numeric matrices, requiring that factor predictors are somehow encoded as numbers. Most often in R, this is in the form of treatment contrasts, where an $n$-length factor with $l$ levels is represented as an $n ~x~ l-1$ matrix composed of zeroes and ones. Each column is referred to as a dummy variable. The first column has value $1$ when the $i$-th entry of the factor is the second level, zero otherwise. The second column has value $1$ when the $i$-th entry of the factor is the third level, zero otherwise. We know that the $i$-th entry of the first took its first level if all of the entries in the $i$th column of the resulting matrix are zero. While this representation of a factor is relatively straightforward, it's quite memory intensive; a factor with 100 levels ultimately will require a 99-column matrix to be allocated in order to be included in a model. While many modeling engines in R assume that factors will be encoded as treatment contrasts, different modeling engines have different approaches to processing factor variables, some more efficient than others. More on this in @sec-sparsity, in particular.

```{r}
#| label: represent-factors-as-dummy
#| echo: false
df <- data.frame(x = factor(letters[1:3]))
mm <- as.data.frame(model.matrix(~x, df))[-1]
```

:::::: columns
::: {.column width="45%"}
Original factor

```{r, echo = FALSE}
df
```
:::

::: {.column width="10%"}
<!-- empty column to create gap -->
:::

::: {.column width="45%"}
Factor with treatment contrasts

```{r, echo = FALSE}
mm
```
:::
::::::

Further, many of those factor variables have a class imbalance; that is, some levels of the factor occur much more often than others. Some models may struggle to learn from the less frequently-occurring classes, potentially requiring more iterations of descent processes for some models to converge. Even when this is not the case, it may be varyingly "worth it" in terms of memory usage to allocate a dummy variable to a factor level that only appears a couple times in a dataset with many rows.

```{r}
#| label: factors-d-class
#| echo: false
#| fig-cap: "Distributions of the categorical variables in the simulated data. While factor levels are well-balanced in the outcome `class`, some factor levels are much more common than others in the predictors, which can lead to instability in many modeling algorithms."
#| fig-alt: "5 bar charts, faceted on top of each other. Each row represents a factor column in the data d, and each column gives a count of observations of each level of the column. The outcome `class` is relatively well-balanced, while each of the predictors have significant class imbalances."
d_class %>%
  select(where(is.factor)) %>%
  summarize(across(everything(), ~list(table(.x)))) %>%
  pivot_longer(cols = everything(), names_to = "variable") %>%
  rowwise() %>%
  mutate(
    level_1 = rlang::try_fetch(value[[1]], error = function(cnd) {NA}),
    level_2 = rlang::try_fetch(value[[2]], error = function(cnd) {NA}),
    level_3 = rlang::try_fetch(value[[3]], error = function(cnd) {NA}),
    level_4 = rlang::try_fetch(value[[4]], error = function(cnd) {NA}),
    level_5 = rlang::try_fetch(value[[5]], error = function(cnd) {NA})
  ) %>%
  select(-value) %>%
  pivot_longer(cols = starts_with("level_"), names_to = "level", values_to = "n") %>%
  ggplot() +
  aes(x = level, y = n) +
  geom_col() +
  facet_grid(rows = vars(variable)) +
  theme_minimal() +
  labs(x = "Factor Level", y = "Number of Observations")
```

<!--# TODO: is it possible to remove the panel grid in the rows/cols that don't apply? -->

The regression dataset looks quite similar.

```{r}
#| label: print-simulate-regression
d_reg <- simulate_regression(1000)

d_reg
```

The left-most column, `outcome`, is a numeric outcome, and the remaining 15 columns are a mix of numeric and factor. The same story related to correlation and tricky factor imbalances goes for the regression dataset. Demonstrating that is homework.

<!--# todo: this isn't "real" data -->
