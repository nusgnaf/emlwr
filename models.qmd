# 模型 {#sec-models}

<!--# 要使这部分内容不显得过于机械，需要做一些工作。 -->

<!--# 我希望有一些内容，比如表格，列出在不同行数的数据上拟合每种___引擎的时间。 -->

::: callout-caution
本章仍有很长的路要走。在此期间，我建议探索草案的其他部分。
:::

## Tidymodels开销

<!--# TODO：当前这部分讨论了CRAN和开发版（dev）包之间的区别。需要重新表述，只关注当前的情况。 -->

\
<!--# 另外，需要重写以适应`eval_fits`的情况和不适用的情况。 -->

```{r}
#| label: setup-common-02
#| include: false
source("includes/common.R")
```

```{r}
#| label: setup-02
#| include: false
if (!identical(Sys.getenv("emlwr.eval_fits"), "true")) {
  eval_fits <- FALSE
} else {
  eval_fits <- TRUE
}

if (!eval_fits) {
  data_files <- list.files("data/models", full.names = TRUE)
  data_files <- data_files[grepl("\\.Rda", data_files)]
  loaded <- lapply(data_files, load)
}
```

虽然tidymodels团队开发了用户直接交互的基础设施，但在底层，我们向其他人的建模包或建模*引擎*发送调用——这些引擎提供了估计参数、生成预测等实际实现。这个过程看起来是这样的：

![一张图表，代表tidymodels界面。按照顺序，第一步“翻译”，第二步“调用”，第三步“翻译”，概述了从标准化的tidymodels界面翻译到引擎特定界面的过程，调用建模引擎，然后再次翻译回标准化的tidymodels界面。第一步和第三步以绿色显示，而第二步以橙色显示。](figures/translate_diagram.png)

当我们考虑上述三个步骤的分配时间时，我们将绿色“翻译”步骤称为*tidymodels开销*。第一步和第三步的“翻译”接口所需的时间在我们的控制之内，而第二步建模引擎所花费的时间则不在。

让我们用一个示例分类问题来演示。生成一些随机数据：

```{r}
#| label: simulate-data
set.seed(1)
d <- simulate_classification(n_rows = 100)

d
```

...我们想使用这个数据集中的其余变量来模拟`class`，使用逻辑回归。我们可以使用以下代码来实现：

```{r}
#| label: fit-glm
fit(logistic_reg(), class ~ ., d)
```

tidymodels中逻辑回归的默认引擎是`stats::glm()`。因此，按照上述图表的风格，这段代码：

1) 将tidymodels代码（在引擎之间是一致的）翻译成特定于所选引擎的格式。在这种情况下，没有太多要做的：它将预处理器作为`formula`传递，数据作为`data`，并选择`stats::binomial`的`family`。
2) 调用`stats::glm()`并收集其输出。
3) 将`stats::glm()`的输出重新翻译成一个标准化的模型拟合对象。

再次强调，我们可以控制第一步和第三步中发生的事情，但第二步属于`stats`包。

第一步和第三步所花费的时间与训练数据的维度相对独立。也就是说，无论我们是在一个一百还是一百万的数据点上训练，*我们的*代码（即，翻译）运行时间大致相同。无论训练集大小如何，我们的代码都会推动小型的关系数据结构，以确定如何正确地与给定引擎接口。然而，第二步所花费的时间几乎完全取决于数据的大小。根据建模引擎的不同，对10倍的数据进行建模可能会导致第二步的时间是原始拟合的两倍、10倍或100倍。

所以，虽然第一步和第三步所分配的*绝对*时间是固定的，但第二步在拟合模型时所占的*部分*时间——即“开销”——取决于引擎代码本身有多快。`glm()`在100个数据点上的逻辑回归有多快？

```{r}
#| label: bench-mark-glm
bench::mark(
  fit = glm(class ~ ., family = binomial, data = d)
) %>% 
  select(expression, median)
```

大约一毫秒。这意味着，如果tidymodels开销是一秒钟，我们使这个模型拟合慢了一千倍！

在实践中，这里的开销在过去几年一直在一两毫秒左右，而机器学习从业者通常拟合的模型比100个数据点上的逻辑回归计算成本要高得多。你只需要相信我关于第二点的说法。关于第一点：

```{r}
bm_logistic_reg <- 
  bench::mark(
    parsnip = fit(logistic_reg(), class ~ ., d),
    stats = glm(class ~ ., family = binomial, data = d),
    check = FALSE
  )
```

记住第一个表达式调用了第二个表达式，所以从第二个到第一个的时间增加是“开销”。在这种情况下，它是`r as.numeric(bm_logistic_reg$median[1] - bm_logistic_reg$median[2]) * 1000`毫秒，或者`r round(1 - as.numeric(bm_logistic_reg$median[2] / bm_logistic_reg$median[1]), 3) * 100`%的总经过时间。

所以，为了在100万个数据点上拟合一个提升树模型，第二步可能需要几秒钟。第一步和第三步不在乎数据的大小，所以它们仍然需要几千分之一秒。没什么大不了的——开销是微不足道的。让我们通过在不同大小的模拟数据集上拟合提升树模型来快速验证这一点，一次使用XGBoost接口，一次使用parsnip的包装器。


```{r}
#| echo: false
#| eval: !expr eval_fits
# default xgboost params from parsnip
params <- list(eta = 0.3, max_depth = 6, gamma = 0, colsample_bytree = 1, 
    colsample_bynode = 1, min_child_weight = 1, subsample = 1)

bm_boost_tree <- 
  bench::press(
    rows = 10^(2:6),
    {
      set.seed(1)
      d <- simulate_classification(rows)
      bt <- boost_tree(mode = "classification")
      bench::mark(
        parsnip = fit(bt, class ~ ., d),
        xgboost = {
          d_mtrx <- model.matrix(class ~ ., d)
          d_xgb <- parsnip:::as_xgb_data(d_mtrx, d[[1]])
          xgboost::xgb.train(params, data = d_xgb$data, nrounds = 15)
        },
        check = FALSE
      )
    }
  )
```

```{r}
#| label: save-bench-press-boost-ree
#| include: false
if (eval_fits) {
  press_boost_tree <- trim_bench_mark(press_boost_tree)
  qsave(press_boost_tree, file = "data/models/press_boost_tree.rds")
} else {
  press_boost_tree <- qread(file = "data/models/press_boost_tree.rds")
}
```

```{r}
#| echo: false
#| label: "plot-press-boost-tree"
#| fig-cap: "TODO: write caption"
press_boost_tree %>%
  mutate(expression = as.character(expression)) %>%
  ggplot() +
  aes(x = rows, y = median, color = expression, group = expression) +
  scale_x_log10() +
  geom_line() +
  labs(x = "Number of Rows", y = "Elapsed Time")
```

这张图表展示了tidymodels在建模引擎上的开销概要：随着数据集大小和模型复杂性的增长，模型拟合和预测在总评估时间中所占的比例越来越大。

@sec-speedy-go展示了用户可以减少他们的tidymodels代码评估时间的多种方法。利用并行计算、减少搜索给定网格所需的模型拟合总数，以及精心构建要搜索的网格，都是故事的主要部分。

## 基准测试

### 线性模型

### 决策树

### 提升树

XGBoost和LightGBM - 对于从Python接口进行的相同操作的比较时间？

### 随机森林

### 支持向量机
